{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train = pd.read_json('data/train.json.zip', orient='columns', convert_dates=['created'], compression='zip')\n",
    "ori_train = ori_train.reset_index()\n",
    "ori_train.rename(columns={'index':'rec_id'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ori_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  pd.read_json('data/test.json.zip', orient='columns', convert_dates=['created'], compression='zip')\n",
    "test = test.reset_index()\n",
    "test.rename(columns={'index':'rec_id'}, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing value & outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers with ridiculously high prices\n",
    "train = train[train['price']<=20000]\n",
    "# remove outliers which the houses locate far away from most of the houses and missing values\n",
    "train = train[(train['latitude'] <= 41.5) & (train['latitude'] >= 40) & (train['longitude'] >= -80) & (train['longitude'] <= -70)]\n",
    "# remove outliers which have bathrooms more than bedrooms\n",
    "train['diff_rooms'] = train['bedrooms'] - train['bathrooms']\n",
    "train = train[train['diff_rooms']>=-1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49165, 74659)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = len(train)\n",
    "test_len = len(test)\n",
    "train_len, test_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train[['manager_id', 'building_id']]\n",
    "test_id = test[['manager_id', 'building_id']]\n",
    "data_id = pd.concat([train_id, test_id], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform manager_id and building_id to numerical variable replace missing values by a uniform \"unknown\" value\n",
    "class_mapping = {label: i for i, label in enumerate(np.unique(data_id['manager_id']))}\n",
    "class_mapping_1 =  {label: i for i, label in enumerate(np.unique(data_id['building_id']))}\n",
    "data_id['manager_id'] = data_id['manager_id'].map(class_mapping)\n",
    "data_id['building_id'] = data_id['building_id'].map(class_mapping_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train[['description', 'features']]\n",
    "test_text = test[['description', 'features']]\n",
    "data_text = pd.concat([train_text, test_text], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # acronym\n",
    "    text = re.sub(r\"br\\s\", \"bedroom\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"cannot\", \"can not \", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'ve \", \" have \", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    # symbol\n",
    "    text = re.sub(r\"&\", \" and \", text)\n",
    "    text = re.sub(r\"\\|\", \" or \", text)\n",
    "    text = re.sub(r\"=\", \" equal \", text)\n",
    "    text = re.sub(r\"\\+\", \" plus \", text)\n",
    "    text = re.sub(r\"\\$\", \" dollar \", text)\n",
    "    # others\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \",text)\n",
    "    # extra \\s\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_text['description_words'] = data_text['description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaggededDocument = gensim.models.doc2vec.TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def addTag(descr):\n",
    "#     train=[]\n",
    "#     for i, doc in enumerate(descr):\n",
    "#         word_list = doc.split(' ')\n",
    "#         document = TaggededDocument(word_list, tags=[i])\n",
    "#         train.append(document)\n",
    "#     return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taggedDoc = addTag(list(data_text['description_words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec = Doc2Vec(taggedDoc, vector_size=100, min_count = 10, window = 5, sample = 1e-5, workers=4)\n",
    "# doc2vec.train(taggedDoc, total_examples=doc2vec.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec.save('model_descrip.model') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_tfidf = TfidfVectorizer(min_df=10, max_features=80, strip_accents='unicode',lowercase =True,\n",
    "                        analyzer='word', token_pattern=r'\\w{5,}', ngram_range=(1, 3),  sublinear_tf=True, stop_words = 'english')  \n",
    "desc_tfidf_fit =desc_tfidf.fit_transform(data_text['description'])\n",
    "desc_name = [x for x in desc_tfidf.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = desc_tfidf_fit.toarray()\n",
    "for i, name in enumerate(desc_name):\n",
    "    list_all = []\n",
    "    [rows, cols] = ar.shape\n",
    "    for row in range(rows):\n",
    "        list_all.append(ar[row][i])\n",
    "    dname = 'desc_' + name\n",
    "    desc_value = pd.Series(list_all,data_text.index, name = dname)\n",
    "    data_text[dname] = desc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(desc_name):\n",
    "    dname = 'desc_' + name\n",
    "    train[dname] = data_text[dname].iloc[:train_len]\n",
    "    test[dname] = data_text[dname].iloc[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_phrase(flist):\n",
    "    plist = []\n",
    "    if len(flist) > 0:\n",
    "        for feature in flist:\n",
    "            feature = re.sub('[_]',' ',feature)\n",
    "            feature = feature.strip()\n",
    "            feature_p = ''.join(feature.split(' '))\n",
    "            plist.append(feature_p)\n",
    "    return plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(flist):\n",
    "    string = ' '.join(flist)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['features_phr'] = data_text['features'].apply(word_to_phrase)\n",
    "data_text['features_phr_str'] = data_text['features_phr'].apply(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "phr_tfidf = TfidfVectorizer(min_df=10, max_features=80, strip_accents='unicode', lowercase=True, token_pattern=r'\\w{3,}', stop_words='english')  \n",
    "phr_tfidf_fit =phr_tfidf.fit_transform(data_text['features_phr_str'])\n",
    "phr_names = [x for x in phr_tfidf.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_feature = phr_tfidf_fit.toarray()\n",
    "for i, name in enumerate(phr_names):\n",
    "    list_all = []\n",
    "    [rows, cols] = ar_feature.shape\n",
    "    for row in range(rows):\n",
    "        list_all.append(ar[row][i])\n",
    "    fname = 'ft_' + name\n",
    "    f_value = pd.Series(list_all, data_text.index, name = fname)\n",
    "    data_text[fname] = f_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(phr_names):\n",
    "    fname = 'ft_' + name\n",
    "    train[fname] = data_text[fname].iloc[:train_len]\n",
    "    test[fname] = data_text[fname].iloc[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add new features to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['manager_id_num'] = data_id['manager_id'].iloc[:train_len]\n",
    "train['building_id_num'] = data_id['building_id'].iloc[:train_len]\n",
    "train['photo_num'] = train['photos'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['manager_id_num'] = data_id['manager_id'].iloc[train_len:]\n",
    "test['building_id_num'] = data_id['building_id'].iloc[train_len:]\n",
    "test['photo_num'] = test['photos'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_feat'] = data_text['features_phr_str'].iloc[:train_len]\n",
    "test['clean_feat'] = data_text['features_phr_str'].iloc[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json('data/new_modified_train.json')\n",
    "test.to_json('data/new_modified_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## additional data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(df):\n",
    "    lat1 = subway['lat']\n",
    "    lat2 = df['latitude']\n",
    "    lon1 = subway['lon']\n",
    "    lon2 = df['longitude']\n",
    "    dlat = np.deg2rad(np.fabs(lat2-lat1))\n",
    "    dlon = np.deg2rad(np.fabs(lon2-lon1))\n",
    "    a = np.sin(dlat/2)**2 + np.cos(np.deg2rad(lat1))*np.cos(np.deg2rad(lat2))*np.sin(dlon/2)**2\n",
    "    c =2 * np.arcsin(np.sqrt(a))\n",
    "    Earth_Radius = 6371\n",
    "    res = min(c * Earth_Radius * 1000)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subway = pd.read_csv('data/subway.csv')\n",
    "subway = subway[['Station Latitude','Station Longitude']]\n",
    "subway = subway.drop_duplicates()\n",
    "subway = subway.rename(columns={'Station Latitude':'lat','Station Longitude':'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['distance_subway'] = train.apply(haversine,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['distance_subway'] = test.apply(haversine,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json('data/new_modified_train.json')\n",
    "test.to_json('data/new_modified_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
